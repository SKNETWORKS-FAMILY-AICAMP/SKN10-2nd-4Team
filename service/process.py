import os
import joblib
import numpy as np
import pandas as pd
from category_encoders import OrdinalEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import KFold, GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

def get_model_and_params(model_name, depth):
    if model_name.lower() == 'rf':
        model = RandomForestClassifier(random_state=42)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [depth, depth + 2, depth + 4],
            'min_samples_split': [2, 5, 10]
        }
    elif model_name.lower() == 'xgb':
        model = XGBClassifier(
            eval_metric='logloss',
            random_state=42,
            min_split_gain=0.0,
            min_child_samples=10
        )
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.1, 0.01, 0.001]
        }
    elif model_name.lower() == 'lgbm':
        model = LGBMClassifier(eval_metric='logloss', random_state=42)
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [depth, depth + 2, depth + 4],
            'learning_rate': [0.1, 0.01, 0.001]
        }
    else:
        raise ValueError("ëª¨ë¸ íƒ€ì…ì€ 'rf', 'xgb', 'lgbm' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    return model, param_grid

def train(args, data, label):
    """
    1) KFold êµì°¨ ê²€ì¦ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ & ì„±ëŠ¥ í‰ê°€ (train+valid ë°ì´í„°)
    2) (ì˜µì…˜) --debug_splitì„ í†µí•´ foldë³„ train/valid ë¶„í¬ í™•ì¸
    """
    kfold = KFold(n_splits=args.split_num, shuffle=True, random_state=42)
    model, param_grid = get_model_and_params(args.model_type, args.depth)

    fold_num = 0
    acc_list = []
    f1_list = []
    best_params_list = []  # ê° foldì˜ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥

    for train_index, valid_index in kfold.split(data):
        fold_num += 1
        X_train, X_valid = data.iloc[train_index], data.iloc[valid_index]
        y_train, y_valid = label.iloc[train_index], label.iloc[valid_index]

        # ë””ë²„ê·¸ ì˜µì…˜: foldë³„ ë ˆì´ë¸” ë¶„í¬ ì¶œë ¥
        if args.debug_split:
            print(f"\n[Fold {fold_num}]")
            print("  Train label distribution:\n", y_train.value_counts())
            print("  Valid label distribution:\n", y_valid.value_counts())

        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy')
        grid_search.fit(X_train, y_train)
        best_model = grid_search.best_estimator_
        best_params_list.append(grid_search.best_params_)

        pred = best_model.predict(X_valid)
        accuracy = np.round(accuracy_score(y_valid, pred), 4)
        f1score = np.round(f1_score(y_valid, pred), 4)
        precision = np.round(precision_score(y_valid, pred), 4)
        recall = np.round(recall_score(y_valid, pred), 4)

        print(f"\n#{fold_num} êµì°¨ ê²€ì¦ ê²°ê³¼")
        print(f"  - Best Params: {grid_search.best_params_}")
        print(f"  - Accuracy: {accuracy}, F1: {f1score}, Precision: {precision}, Recall: {recall}")
        print(f"  - í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape[0]}, ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_valid.shape[0]}")

        acc_list.append(accuracy)
        f1_list.append(f1score)

    print("\n## í‰ê·  ê²€ì¦ Accuracy:", np.mean(acc_list))
    print("## í‰ê·  ê²€ì¦ F1-score:", np.mean(f1_list))

    return best_params_list

# def final_evaluation(args, train_valid_data, train_valid_label, test_data, test_label, best_params_list):
#     final_params = best_params_list[-1]
#     model, _ = get_model_and_params(args.model_type, args.depth)
    
#     if args.model_type.lower() == 'xgb':
#         final_model = XGBClassifier(**final_params, eval_metric='logloss', random_state=42, min_split_gain=0.0, min_child_samples=10)
#     elif args.model_type.lower() == 'lgbm':
#         final_model = LGBMClassifier(**final_params, eval_metric='logloss', random_state=42)
#     else:
#         final_model = RandomForestClassifier(**final_params, random_state=42)
    
#     final_model.fit(train_valid_data, train_valid_label)
#     pred_test = final_model.predict(test_data)
#     acc_test = accuracy_score(test_label, pred_test)
#     f1_test = f1_score(test_label, pred_test)
#     precision_test = precision_score(test_label, pred_test)
#     recall_test = recall_score(test_label, pred_test)
    
#     print("\n## ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€ ê²°ê³¼")
#     print(f"  - Accuracy: {acc_test}")
#     print(f"  - F1-score: {f1_test}")
#     print(f"  - Precision: {precision_test}")
#     print(f"  - Recall: {recall_test}")

#     # ëª¨ë¸ ì €ì¥
#     os.makedirs("model", exist_ok=True)
#     model_path = os.path.join("model", f"final_model_{args.model_type}.pkl")
#     joblib.dump(final_model, model_path)
#     print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

def final_evaluation(args, train_valid_data, train_valid_label, test_data, test_label, best_params, cluster_num):
    final_params = best_params[-1]
    model, _ = get_model_and_params(args.model_type, args.depth)
    
    if args.model_type.lower() == 'xgb':
        final_model = XGBClassifier(**final_params, eval_metric='logloss', random_state=42, min_split_gain=0.0, min_child_samples=10)
    elif args.model_type.lower() == 'lgbm':
        final_model = LGBMClassifier(**final_params, eval_metric='logloss', random_state=42)
    else:
        final_model = RandomForestClassifier(**final_params, random_state=42)
    
    final_model.fit(train_valid_data, train_valid_label)
    pred_test = final_model.predict(test_data)
    acc_test = accuracy_score(test_label, pred_test)
    f1_test = f1_score(test_label, pred_test)
    precision_test = precision_score(test_label, pred_test)
    recall_test = recall_score(test_label, pred_test)
    
    print(f"\n## í´ëŸ¬ìŠ¤í„° {cluster_num} ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€ ê²°ê³¼")
    print(f"  - Accuracy: {acc_test}")
    print(f"  - F1-score: {f1_test}")
    print(f"  - Precision: {precision_test}")
    print(f"  - Recall: {recall_test}")

    # ëª¨ë¸ ì €ì¥ (í´ëŸ¬ìŠ¤í„°ë³„ ëª¨ë¸)
    os.makedirs("model", exist_ok=True)
    model_path = os.path.join("model", f"Cluster{cluster_num}_model_{args.model_type}.pkl")
    joblib.dump(final_model, model_path)
    print(f"âœ… í´ëŸ¬ìŠ¤í„° {cluster_num} ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")


# def run_full_process(args):
#     print("ğŸš€ ë°ì´í„° ë¡œë“œ ì¤‘...")
#     df = pd.read_csv("data/clustered_Customer_Churn.csv")
#     X = df.drop('Exited', axis=1)
#     y = df['Exited']
#     print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    
#     print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
#     best_params = train(args, X, y)
#     print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
    
#     print("ğŸš€ ìµœì¢… ëª¨ë¸ í‰ê°€...")
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#     final_evaluation(args, X_train, y_train, X_test, y_test, best_params)
    
#     return best_params

def run_full_process(args):
    print("ğŸš€ ë°ì´í„° ë¡œë“œ ì¤‘...")

    best_params_dict = {}  # í´ëŸ¬ìŠ¤í„°ë³„ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
    
    for cluster_num in [0, 3]:
        file_path = f"data/Cluster_{cluster_num}_Preprocessed.csv"
        
        if not os.path.exists(file_path):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}, í´ëŸ¬ìŠ¤í„° {cluster_num} ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue
        
        print(f"\nğŸ“Œ í´ëŸ¬ìŠ¤í„° {cluster_num} ë°ì´í„° ë¡œë“œ ì¤‘...")
        df = pd.read_csv(file_path)
        
        if "Exited" not in df.columns:
            print(f"âŒ 'Exited' ì»¬ëŸ¼ ì—†ìŒ: {file_path}")
            continue
        
        X = df.drop('Exited', axis=1)
        y = df['Exited']
        print(f"âœ… í´ëŸ¬ìŠ¤í„° {cluster_num} ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ìƒ˜í”Œ ìˆ˜: {X.shape[0]})")
        
        print(f"ğŸš€ í´ëŸ¬ìŠ¤í„° {cluster_num} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        best_params = train(args, X, y)
        best_params_dict[cluster_num] = best_params
        print(f"âœ… í´ëŸ¬ìŠ¤í„° {cluster_num} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        
        print(f"ğŸš€ í´ëŸ¬ìŠ¤í„° {cluster_num} ìµœì¢… ëª¨ë¸ í‰ê°€...")
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        final_evaluation(args, X_train, y_train, X_test, y_test, best_params, cluster_num)
    
    return best_params_dict
